{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP MNMG 3D visualization of Google News word2vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please first read https://plotly.com/python/getting-started to properly install the Plotly library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.manifold import UMAP as UMAP_MNMG\n",
    "from cuml.manifold import UMAP\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GoogleNews word2vec dataset and extract embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "original_data = wv_from_bin.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(original_data, sampling_ratio, n_parts):\n",
    "    # Number of samples\n",
    "    n_samples = original_data.shape[0]\n",
    "    \n",
    "    # Number of samples for local train\n",
    "    n_sampling = int(n_samples * sampling_ratio)\n",
    "    \n",
    "    # Generate local train data\n",
    "    selection = np.random.choice(n_samples, n_sampling)\n",
    "    lX = original_data[selection]\n",
    "    \n",
    "    # Number of samples per partition\n",
    "    n_samples_per_part = int(n_samples / n_parts)\n",
    "    \n",
    "    # Generate partitioning of distributed data for inference\n",
    "    chunks = [n_samples_per_part] * n_parts\n",
    "    chunks[-1] += n_samples % n_samples_per_part\n",
    "    chunks = tuple(chunks)\n",
    "    dX = da.from_array(original_data, chunks=(chunks, -1))\n",
    "    return lX, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_ratio = 0.05 # 5% of samples used for training\n",
    "n_parts = 8 # 8 data split in 8 partitions, also 8 workers/GPUs\n",
    "\n",
    "# Prepare local train data and distributed inference data\n",
    "lX, dX = prepare_data(original_data, sampling_ratio, n_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train local model\n",
    "local_model = UMAP(n_components=3)\n",
    "local_model.fit(lX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Dask client\n",
    "cluster = LocalCUDACluster(n_workers=8, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "# Pass trained model and compute distributed inference\n",
    "model = UMAP_MNMG(local_model)\n",
    "transformed = model.transform(dX).compute()\n",
    "\n",
    "# Stop Dask client\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with words and associated embeddings\n",
    "result = pd.DataFrame({'dim1': transformed[:, 0], 'dim2': transformed[:, 1], 'dim3': transformed[:, 2]})\n",
    "result['word'] = wv_from_bin.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating backup\n",
    "import pickle\n",
    "pickle.dump(result, open(\"gnews-embedding.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading backup\n",
    "import pickle\n",
    "result = pickle.load(open(\"gnews-embedding.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# 3D visualization of 500 words\n",
    "fig = px.scatter_3d(result.head(500), x='dim1', y='dim2', z='dim3', text='word')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
