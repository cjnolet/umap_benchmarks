{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP MNMG benchmark (runtime & trustworthiness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.manifold import UMAP as UMAP_MNMG\n",
    "from cuml.manifold import UMAP\n",
    "from cuml.metrics import trustworthiness\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(args):\n",
    "    # Generate dataset\n",
    "    X, y = make_blobs(n_samples=args['n_samples'], n_features=args['n_features'],\n",
    "                      centers=args['centers'])\n",
    "    \n",
    "    # Number of samples for local train\n",
    "    n_sampling = int(args['n_samples'] * args['sampling_ratio'])\n",
    "    \n",
    "    # Generate local train data\n",
    "    selection = np.random.choice(args['n_samples'], n_sampling)\n",
    "    lX = X[selection]\n",
    "    \n",
    "    # Number of samples per partition\n",
    "    n_samples_per_part = int(args['n_samples'] / args['n_parts'])\n",
    "    \n",
    "    # Generate partitioning of distributed data for inference\n",
    "    chunks = [n_samples_per_part] * args['n_parts']\n",
    "    chunks[-1] += args['n_samples'] % n_samples_per_part\n",
    "    chunks = tuple(chunks)\n",
    "    dX = da.from_array(X, chunks=(chunks, -1))\n",
    "\n",
    "    # Warm up (used to prevent statistical anomalies in time measurement due to first time initialization)\n",
    "    local_model = UMAP(n_components=2, n_neighbors=args['n_neighbors'],\n",
    "                       n_epochs=args['n_epochs'], random_state=args['random_state'])\n",
    "    local_model.fit(lX)\n",
    "    model = UMAP_MNMG(local_model)\n",
    "    model.transform(dX).compute()\n",
    "    \n",
    "    # Measure and average runtime and trustworthiness accross multiple runs\n",
    "    durations = []\n",
    "    trust_scores = []\n",
    "    for i in range(args['n_iter']):\n",
    "        \n",
    "        # Train local model\n",
    "        local_model = UMAP(n_components=2, n_neighbors=args['n_neighbors'],\n",
    "                       n_epochs=args['n_epochs'], random_state=args['random_state'])\n",
    "        local_model.fit(lX)\n",
    "        \n",
    "        # Pass trained model and order distributed inference\n",
    "        model = UMAP_MNMG(local_model)\n",
    "        lazy_transformed = model.transform(dX)\n",
    "        \n",
    "        # Perform distributed inference and measure time\n",
    "        start = time.time()\n",
    "        transformed = lazy_transformed.compute()\n",
    "        durations.append(time.time()-start)\n",
    "        \n",
    "        # Compute trustworthiness score\n",
    "        trust_scores.append(trustworthiness(X, transformed, n_neighbors=args['n_neighbors']))\n",
    "        \n",
    "    durations = np.array(durations)\n",
    "    trust_scores = np.array(trust_scores)\n",
    "    \n",
    "    # Return runtime average and variance as well as trustworthiness score average\n",
    "    return durations.mean(), durations.var(), trust_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters definitions :\n",
    "- **n_samples** : number of samples\n",
    "- **n_features** : number of features\n",
    "- **centers** : number of blobs to generate the dataset\n",
    "- **n_neighbors** : number of neighbors used to generate fuzzy simplicial set in UMAP\n",
    "- **n_epochs** : number of iterations during UMAP optimization step\n",
    "- **random_state** : random seed used in UMAP\n",
    "- **n_parts** : number of partitions into which the dataset is divided, also number of workers/GPUs to be used\n",
    "- **sampling_ratio** : ratio of samples used during local training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "args = {'n_features':100, 'centers':300,\n",
    "        'n_neighbors':15, 'n_epochs':5000, 'random_state': 42,\n",
    "        'sampling_ratio':0.1, 'n_iter': 3}\n",
    "\n",
    "# Start Dask client\n",
    "cluster = LocalCUDACluster(n_workers=8, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "# Comparing runtime and trustworthiness with different number of partitions, number of samples and ratio of local train\n",
    "for n_parts in [3, 8]:\n",
    "    for n_samples in [100000, 500000, 1000000]:\n",
    "        for sampling_ratio in [0.001,0.005]:\n",
    "            args['n_parts'] = n_parts\n",
    "            args['n_samples'] = n_samples\n",
    "            args['sampling_ratio'] = sampling_ratio\n",
    "            duration_mean, duration_var, trust = benchmark(args)\n",
    "            print(\"n_parts: {}, n_samples: {}, sampling_ratio: {}, duration avg - var: {:.2f} - {:.2f}, tustworthiness: {:.2f}\".format(n_parts, n_samples,\n",
    "                                                                                                        sampling_ratio, duration_mean, duration_var, trust))\n",
    "# Stop Dask client\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
